%
% Copyright (c) 2020 Antonio Co√≠n Castro
%
% This work is licensed under a
% Creative Commons Attribution-ShareAlike 4.0 International License.
%
% You should have received a copy of the license along with this
% work. If not, see <http://creativecommons.org/licenses/by-sa/4.0/>.

All the code developed for this bachelor's thesis is freely available at \url{https://github.com/antcc/fuzzyspark} under a GPLv3 license. It consists roughly on 2200 lines of code written in Scala 2.11.8 and Spark 2.3.2. Every function, class and method is properly documented in the source code.

The algorithms developed come bundled as a package, called \textit{fuzzyspark}, which consists on a subpackage for the clustering algorithms and another one for more generic FRBSs. In addition, a couple of separate testing files are provided with the code used to perform the comparative study on Chapter \ref{ch:study}. The project can be compiled using Scala's interactive building tool, \textit{sbt}, and two build files are provided (one for the algorithms suite and one for the testing files). Since the package developed is not published anywhere, some additional steps have to be taken:

\begin{enumerate}[1.]
  \item In the first place, the package \textit{fuzzyspark} needs to be published to the local repository, with the order \verb|sbt publishLocal| inside the \textit{fuzzyspark} folder.
  \item In the second place, the published package needs to be bundled with the testing application as a dependency, for which we can use the \textit{assembly} plugin, executing the order \verb|sbt assembly| inside the \textit{testing} directory.
\end{enumerate}
Following the above steps we end up with a JAR file ready to be executed in Spark. For this execution we use the script \textit{spark-submit} that comes with Spark. In particular, a JAR file containing Spark code can be executed with the following line, where $N$, $M$ and $C$ are adjustable parameters to specify the configuration of the YARN cluster:

\begin{verbatim}
spark-submit --class MAIN_CLASS --num-executors N --executor-cores C \
  --executor-memory M [JAR_FILE] [ARGS]
\end{verbatim}

Regarding the file structure, two files have been developed for each algorithm: one which contains the learning phase and methods to fit the model to the data available, and another which contains the trained model and implements the prediction phase. Specifically, they are:

\begin{itemize}
  \item \verb|FuzzyCMeans.scala| and \verb|FuzzyCMeansModel.scala| for the Fuzzy C-Means algorithm.
  \item \verb|WM.scala| and \verb|WMModel.scala| for the Wang and Mendel algorithm.
  \item \verb|SubtractiveClustering.scala| and \verb|ModelIdentification.scala| for the subtractive clustering algorithm and its variants.
\end{itemize}
As we mentioned earlier, there are two additional files to test the implementations; they are called \verb|ClusteringTest.scala| and \verb|ModelTest.scala|. There is also another file, \verb|Utils.scala|, that comprises some utility methods.

No external libraries apart from Spark were used in the code. The principal data structures used are of course Spark's RDDs, and each data point was modelled as a \verb|Vector| of floating point values.

Finally, we summarize some of the difficulties that arose in the programming phase, some of them having to do with how Spark works internally. We found that iterative algorithms are still a challenge to properly implement in Spark. For example, we observed that when we modified an RDD on each iteration and the number of iterations grew large, a special operation called a \textit{checkpoint} needed to be performed in order to restart the \textit{lineage} of the RDD (that is, the history of the set of transformations executed upon it), or else memory problems would occur. Also, the data should always be cached so that it is kept in memory between iterations to increase performance. In some cases we also needed to tune a few of Spark parameters (such as the garbage collector or the maximum response time for the worker nodes) to prevent some errors in the execution. The number of partitions was also an essential factor that greatly influenced execution time.
