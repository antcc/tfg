%
% Copyright (c) 2020 Antonio Coín Castro
%
% This work is licensed under a
% Creative Commons Attribution-ShareAlike 4.0 International License.
%
% You should have received a copy of the license along with this
% work. If not, see <http://creativecommons.org/licenses/by-sa/4.0/>.

Having concluded the exposition of the work we set out to do in the beginning to explore the interaction between fuzzy systems and Big Data, we can confidently assert that we have completed it satisfactorily. We will now bring this study to an end by summarizing the main tasks accomplished and the conclusions drawn from the various parts of this work.

In the first place, we introduced the theory of fuzzy systems from the ground up, reviewing the basics concepts of fuzzy set theory and fuzzy logic from a theoretical point of view, while never losing sight of the possible practical applications implied therein. Of special significance is the analysis of the process of fuzzy inference and fuzzy reasoning, and in particular of the fuzzy \textit{if-then} rules, whose implications are clearly reflected on our practical work.

In the theoretical realm we also studied the concept of Big Data, its main characteristics and a plethora of aspects to take into account when dealing with this emerging field of computer engineering and data analysis. The main conclusion extracted from this disquisition is that new technologies and infrastructures are needed to keep up with the enormous quantity of data that we are constantly generating. Moreover, this shift in perspective is here to stay and will soon be the norm and not the exception, so it is of the utmost importance to develop robust algorithms that can help process and analyze such large amounts of data. In addition to this descriptive exposition, we have explored the ethical implications that come with the gathering, processing and eventual application of personal information. This is a topic we feel that, while being tangential to our study, merits a profound reflection as well. Though brief, in our discussion about Big Data ethics we concluded that the individuals should own their data and have a say in how it is utilized. They also need to be protected so that this remains so: it should always be the case that data is used \textit{for} the subjects and not \textit{against} them. Together with this exposition we presented a review of the structure of Spark and the MapReduce programming model, which is considered to be the state-of-the-art framework as far as distributed programming is concerned.

From the implementation task we have undertaken there are also some valuable conclusions to be extracted.

\begin{itemize}
  \item The adaptation of the usual algorithms to a Big Data environment cannot be a mere parallelization of the sequential version. In most cases a careful planning of a scalable design is needed to succeed in this endeavour, or else the results will most likely be disappointing. We experienced this situation in the first algorithm developed, for which we needed to implement three versions until the results were admissible time-wise. Nevertheless, in the end we developed three fuzzy learning algorithms that are scalable and can be readily used in any Big Data problem, which was the main goal of this practical portion of the work.
  \item Fuzzy systems are traditionally used as the backbone of control systems and other processes that require a high level of interpretability. By fusing this approach with the scalability that comes with an algorithm designed specifically to handle big amounts of data, we have reached a compromise in which we can increase the information available to learn while maintaining some notion of how this information is learned and reflected in the final model. This contrasts with some celebrated models such as \textit{deep neural networks}, which act almost like a black box that receives an input and produces an output in some intricate way.
  \item The fact that we used a framework of reference such as Spark to implement the algorithms referred to in this work results in a more easily maintainable and future-proof implementation, with plenty of documentation and libraries available. A sufficient knowledge of the Scala programming language, of the Spark paradigm and of the interaction between them was needed before the implementation could begin. For this reason we dedicated a considerable amount of time to this learning effort, and as a side effect we began to develop a certain affinity for a more functional style of programming. However, some difficulties did arise while completing the implementation, and a more in-depth study into the inner workings of Spark was needed in order to overcome some of them. A brief description of these mishaps is presented in Appendix \ref{app:software}.
\end{itemize}

Finally, we explore a few options for future work on this topic. The first and more straightforward path would be to fully adapt the implementation of the algorithms developed to integrate them in Spark's MLlib. The second idea would be to build upon our implementations and develop a complete suite of scalable fuzzy rule-based systems, providing a common interface and improving their easiness of use. Lastly, an interesting approach would be to inquire into the interpretability of these systems and analyze to what extent the fuzzy rules that stem from them lend themselves to be interpreted by humans, exploring the so-called \textit{accuracy-interpretability trade-off}. In this regard a good starting point for research would be the works of Alcalá \textit{et al.} \cite{hybrid2006alcala,gacto2011interpretability}.
